{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JV5YbbVNlCLl"
   },
   "source": [
    "# Advanced Automation 2025/26 - Assignment 1\n",
    "\n",
    "To be delivered until 2025-12-05 23:59:59.\n",
    "\n",
    "**Submission Notes**:\n",
    "- Create a folder in your group's GitHub repository to solve this assignment. Copy this notebook into that folder.\n",
    "- You should commit regularly to your repository the answers to the questions in this notebook. If you do not, your grade will be penalized by 1/20 points.\n",
    "- After running the entire notebook (including graphs and outputs), save the notebook as a .pdf file, by going to File - Print - Destination: Save as PDF.\n",
    "- Create a .zip file containing both the .ipynb file (the notebook itself) and the .pdf and submit it in Fénix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9Fxf5EgMPkm"
   },
   "source": [
    "# Problem description\n",
    "\n",
    "The dataset for this assignment contains information on students from two Portuguese secondary schools enrolled in a mathematics course.\n",
    "Each row corresponds to one student and includes demographic and family background variables (e.g., basic personal and household characteristics), school-related variables (e.g., school context, study habits, past failures), behavioral and lifestyle indicators (e.g., free time, going out, alcohol use), and the grades for the three school periods, on a 0–20 scale.\n",
    "\n",
    "The goal is to predict the performance of the students in the final period.\n",
    "First, the problem will be approached as a classification task, predicting whether a student will pass (G3 >= 10) or fail (G3 < 10) the course.\n",
    "Then, the problem will be approached as a regression task, predicting the actual final grade (G3).\n",
    "\n",
    "A detailed description of all variables (family context, behavioral, school performance, etc.) is provided in the file `variable_description.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HE6Xg8LBMPkn",
    "outputId": "f32218ef-0cf9-4982-fd23-bf36fb21b8a7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set pandas display options for better viewing\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"Libraries imported and display options set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xhgIt6aMPkq"
   },
   "source": [
    "# Part 1 - Data exploration **(4.00)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Euzx5mWaMPkq"
   },
   "source": [
    "##### **1.1.** Load the dataset from the CSV file `mathematics_grades.csv`. **(0.25)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "Tn4S1FIBMPkr",
    "outputId": "7cbc9f31-620c-4d0d-9430-a9c5e319f870"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('mathematics_grades.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNDt2OwMMPkr"
   },
   "source": [
    "##### **1.2.** Identify the feature types of each variable in the dataset. **(1.50)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QA0_1iZ_MPks",
    "outputId": "38376bc3-5aa3-4ee0-e21e-b4a4a49e8262"
   },
   "outputs": [],
   "source": [
    "print(\"DataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nDataFrame dtypes:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Separating numerical and categorical columns for a clearer view\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "print(\"\\nNumerical Columns:\")\n",
    "print(numerical_cols)\n",
    "\n",
    "print(\"\\nCategorical Columns:\")\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDvB2LOyMPks"
   },
   "source": [
    "##### **1.3** Check if the dataset has missing values. If so, discard any row that has a missing value. **(0.25)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgPdmjyJNO3l",
    "outputId": "68c92705-dc0a-4b4d-e9f3-55eeb07b348e"
   },
   "outputs": [],
   "source": [
    "print(\"Missing values before handling:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check if there are any missing values\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    print(\"\\nMissing values detected. Dropping rows with missing values...\")\n",
    "    initial_rows = df.shape[0]\n",
    "    df.dropna(inplace=True)\n",
    "    rows_after_dropping = df.shape[0]\n",
    "    print(f\"Dropped {initial_rows - rows_after_dropping} rows with missing values.\")\n",
    "else:\n",
    "    print(\"\\nNo missing values detected in the dataset.\")\n",
    "\n",
    "print(\"\\nMissing values after handling:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nDataFrame shape after handling missing values: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkYeObBKMPkt"
   },
   "source": [
    "##### **1.4.** Make a pairplot of the numerical variables, colored by the `failed` variable. **(0.50)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "_1aMhv6TMPku",
    "outputId": "7eaadf02-2b66-4802-8571-c1452409c803"
   },
   "outputs": [],
   "source": [
    "# Select only numerical columns for the pairplot, and include the 'failed' column for coloring\n",
    "plot_df = df[numerical_cols + ['failed']]\n",
    "\n",
    "sns.pairplot(plot_df, hue='failed', palette='coolwarm')\n",
    "plt.suptitle('Pairplot of Numerical Variables colored by Failed Status', y=1.02) # Adjust suptitle position\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pFwgo4QMPku"
   },
   "source": [
    "##### **1.5.** Make bar plots for each categorical variable, showing the counts for each category, colored by the `failed` variable. **(0.50)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bE0AbawgMPku",
    "outputId": "29cb5a54-a539-4ba4-95d3-a51b53eae429"
   },
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x=col, hue='failed', palette='viridis')\n",
    "    plt.title(f'Counts of {col} by Failed Status')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ERx6xyhMPku"
   },
   "source": [
    "##### **1.6.** In each school, how many students live in a rural area and do not have internet access at home? **(0.50)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVdsBBQMMPku",
    "outputId": "dda3a6d1-a2d8-41c2-bf34-248c93a2dc94"
   },
   "outputs": [],
   "source": [
    "# Filter for students living in a rural area ('R') and without internet access ('no')\n",
    "rural_no_internet_students = df[(df['address'] == 'R') & (df['internet'] == 'no')]\n",
    "\n",
    "# Group by 'school' and count the number of students\n",
    "counts_by_school = rural_no_internet_students['school'].value_counts().reset_index()\n",
    "counts_by_school.columns = ['School', 'Number of Students']\n",
    "\n",
    "print(\"Number of students living in a rural area and without internet access, per school:\")\n",
    "print(counts_by_school)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bANYgg8SMPkv"
   },
   "source": [
    "##### **1.7.** Split the dataset into a training set (80%) and a test set (20%). **(0.50)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZykcVVBMPkv",
    "outputId": "f27ceedc-854f-4b7d-8298-be3d033b5fee"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop('failed', axis=1)\n",
    "y = df['failed']\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Original DataFrame shape: {df.shape}\")\n",
    "print(f\"Training set features shape: {X_train.shape}\")\n",
    "print(f\"Testing set features shape: {X_test.shape}\")\n",
    "print(f\"Training set target shape: {y_train.shape}\")\n",
    "print(f\"Testing set target shape: {y_test.shape}\")\n",
    "\n",
    "print(\"\\nDistribution of 'failed' in original data:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "print(\"\\nDistribution of 'failed' in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nDistribution of 'failed' in test set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7qixBfAMPkv"
   },
   "source": [
    "# Part 2 - Classification **(7.50)**\n",
    "\n",
    "##### You'll start by approaching the problem as a classification task, predicting whether a student will pass (G3 >= 10) or fail (G3 < 10) the course. Consider the variable `failed` as the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pb_LQ1xQMPkw"
   },
   "source": [
    "##### **2.1.** Which performance metric do you think are most important for this prediction task, and why? Justify your answer in the context of supporting students who may struggle academically. **(2.00)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30054f4c",
    "outputId": "e76c7532-20cb-4432-b589-284856d9c646"
   },
   "outputs": [],
   "source": [
    "with open('feature_descriptions.txt', 'r') as f:\n",
    "    feature_descriptions = f.read()\n",
    "print(feature_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boYIHj7kMPkw"
   },
   "source": [
    "A métrica mais importante é a sensibilidade, ou taxa de verdadeiros positivos. Isto porque ao identificar corretamente alunos que chumbem os apoios são fornecidos a estes com mais necessidades. Reduzimos também os falsos negativos que neste caso representam alunos que segundo o modelo iriam passar mas na verdade chumbam. Portanto neste caso ao termos uma sensibilidade alta indica que o modelo identifica corretamente os alunos que iram chumbar. E isto é crucial para estes casos visto que é um erro mais crítico falhar a identificar alguém que chumbe, do que identificar um aluno que passe como alguém que chumbe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbU6tEKiMPkw"
   },
   "source": [
    "##### **2.2.** Build a logistic regression model to predict whether a student will pass or fail the course, using as predictor the feature `absences`. Evaluate the performance of the model on the test set using a confusion matrix, accuracy, precision, recall, f1-score and the metric you selected in question 2.1. **(0.50)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "id": "b2suG9yqMPkw",
    "outputId": "195ce87f-43b7-4967-87fd-359b698645b7"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the feature (absences) for training\n",
    "# Reshape X_train_absences and X_test_absences to be 2D arrays as expected by sklearn models\n",
    "X_train_absences = X_train[['absences']]\n",
    "X_test_absences = X_test[['absences']]\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "# Using 'liblinear' solver for small datasets and 'balanced' class_weight to handle potential imbalance\n",
    "model_absences = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42)\n",
    "model_absences.fit(X_train_absences, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_absences = model_absences.predict(X_test_absences)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(\"\\n--- Model Performance Evaluation (using 'absences') ---\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_absences)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Display the Confusion Matrix visually\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Pass (0)', 'Fail (1)'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix - Absences Predictor\")\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_absences)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Precision (for the 'failed' class, which is True or 1)\n",
    "precision = precision_score(y_test, y_pred_absences, pos_label=True)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Recall (for the 'failed' class, which is True or 1) - Metric selected in 2.1\n",
    "recall = recall_score(y_test, y_pred_absences, pos_label=True)\n",
    "print(f\"Recall (Selected Metric): {recall:.4f}\")\n",
    "\n",
    "# F1-score\n",
    "f1 = f1_score(y_test, y_pred_absences, pos_label=True)\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation of Confusion Matrix:\")\n",
    "print(f\"  True Negatives (Correctly predicted as passing): {cm[0, 0]}\")\n",
    "print(f\"  False Positives (Predicted as failing, but actually passed): {cm[0, 1]}\")\n",
    "print(f\"  False Negatives (Predicted as passing, but actually failed): {cm[1, 0]}\")\n",
    "print(f\"  True Positives (Correctly predicted as failing): {cm[1, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idTQV-KMMPkx"
   },
   "source": [
    "##### **2.3.** The model you built in question 2.2 likely does not perform very well. Explain why this is the case, considering the distribution of the predictor and the response. How can you improve the model? **(2.00)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjRbiPouMPkx"
   },
   "source": [
    "O fraco desempenho do modelo deve-se fundamentalmente à limitação do preditor único (absences), o qual é insuficiente para prever o insucesso escolar, um resultado complexo influenciado por múltiplos fatores como notas anteriores e apoio familiar. A Regressão Logística é incapaz de modelar eficazmente esta relação fraca e potencialmente não linear. Consequentemente, o modelo exibe um baixo Recall de 0.4211, falhando em identificar a maioria dos alunos que realmente chumbam, o que se traduz num elevado número de Falsos Negativos. Para melhorar a performance, é crucial incluir mais características preditivas (como G1, G2, failures e studytime), tratar corretamente as variáveis categóricas, e migrar para modelos mais robustos (como Random Forests ou Gradient Boosting Machines), ajustando-se, por fim, o limiar de classificação para priorizar o Recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-weUz-ADMPkx"
   },
   "source": [
    "##### **2.4.** Consider now the variable `studytime` as the predictor and assume it is numerical. Build a logistic regression model to predict whether a student will pass or fail the course. Evaluate the performance of the model on the test set using a confusion matrix, accuracy, precision, recall, f1-score and the metric you selected in question 2.1. **(0.50)**\n",
    "\n",
    "Note: Use `sklearn.linear_model.LogisticRegression(class_weight='balanced')` for your logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "id": "T1BVHAhuMPky",
    "outputId": "70465635-d8aa-4f79-9a85-966f8186a740"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the feature (studytime) for training, assuming it is numerical\n",
    "X_train_studytime_numerical = X_train[['studytime']]\n",
    "X_test_studytime_numerical = X_test[['studytime']]\n",
    "\n",
    "# Initialize and train the Logistic Regression model with balanced class weights\n",
    "model_studytime_numerical = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42)\n",
    "model_studytime_numerical.fit(X_train_studytime_numerical, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_studytime_numerical = model_studytime_numerical.predict(X_test_studytime_numerical)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(\"\\n--- Model Performance Evaluation (using numerical 'studytime') ---\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_num = confusion_matrix(y_test, y_pred_studytime_numerical)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_num)\n",
    "\n",
    "# Display the Confusion Matrix visually\n",
    "disp_num = ConfusionMatrixDisplay(confusion_matrix=cm_num, display_labels=['Pass (0)', 'Fail (1)'])\n",
    "disp_num.plot(cmap=plt.cm.Greens)\n",
    "plt.title(\"Confusion Matrix - Numerical Studytime Predictor\")\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy_num = accuracy_score(y_test, y_pred_studytime_numerical)\n",
    "print(f\"\\nAccuracy: {accuracy_num:.4f}\")\n",
    "\n",
    "# Precision\n",
    "precision_num = precision_score(y_test, y_pred_studytime_numerical, pos_label=True)\n",
    "print(f\"Precision: {precision_num:.4f}\")\n",
    "\n",
    "# Recall (for the 'failed' class, which is True or 1) - Metric selected in 2.1\n",
    "recall_num = recall_score(y_test, y_pred_studytime_numerical, pos_label=True)\n",
    "print(f\"Recall (Selected Metric): {recall_num:.4f}\")\n",
    "\n",
    "# F1-score\n",
    "f1_num = f1_score(y_test, y_pred_studytime_numerical, pos_label=True)\n",
    "print(f\"F1-score: {f1_num:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation of Confusion Matrix:\")\n",
    "print(f\"  True Negatives (Correctly predicted as passing): {cm_num[0, 0]}\")\n",
    "print(f\"  False Positives (Predicted as failing, but actually passed): {cm_num[0, 1]}\")\n",
    "print(f\"  False Negatives (Predicted as passing, but actually failed): {cm_num[1, 0]}\")\n",
    "print(f\"  True Positives (Correctly predicted as failing): {cm_num[1, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpXEVNtxMPky"
   },
   "source": [
    "##### **2.5.** Repeat question 2.4, but now considering `studytime` as a categorical variable. **(0.50)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "id": "TH1P-7s_MPkz",
    "outputId": "2a752857-7661-4000-c5b1-251ce30d7aea"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the feature (studytime) for training, treating it as categorical\n",
    "# Use OneHotEncoder to convert 'studytime' into categorical features\n",
    "\n",
    "# Create a OneHotEncoder instance\n",
    "# handle_unknown='ignore' will allow the encoder to ignore categories not seen during fit\n",
    "# sparse_output=False ensures a dense array output\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Fit the encoder on the training data and transform both train and test data\n",
    "X_train_studytime_categorical = encoder.fit_transform(X_train[['studytime']])\n",
    "X_test_studytime_categorical = encoder.transform(X_test[['studytime']])\n",
    "\n",
    "# Create DataFrames with column names for better readability (optional, but good practice)\n",
    "# Get feature names from the encoder\n",
    "feature_names = encoder.get_feature_names_out(['studytime'])\n",
    "X_train_studytime_categorical_df = pd.DataFrame(X_train_studytime_categorical, columns=feature_names, index=X_train.index)\n",
    "X_test_studytime_categorical_df = pd.DataFrame(X_test_studytime_categorical, columns=feature_names, index=X_test.index)\n",
    "\n",
    "\n",
    "# Initialize and train the Logistic Regression model with balanced class weights\n",
    "model_studytime_categorical = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42)\n",
    "model_studytime_categorical.fit(X_train_studytime_categorical_df, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_studytime_categorical = model_studytime_categorical.predict(X_test_studytime_categorical_df)\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(\"\\n--- Model Performance Evaluation (using categorical 'studytime') ---\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_cat = confusion_matrix(y_test, y_pred_studytime_categorical)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_cat)\n",
    "\n",
    "# Display the Confusion Matrix visually\n",
    "disp_cat = ConfusionMatrixDisplay(confusion_matrix=cm_cat, display_labels=['Pass (0)', 'Fail (1)'])\n",
    "disp_cat.plot(cmap=plt.cm.Purples)\n",
    "plt.title(\"Confusion Matrix - Categorical Studytime Predictor\")\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "accuracy_cat = accuracy_score(y_test, y_pred_studytime_categorical)\n",
    "print(f\"\\nAccuracy: {accuracy_cat:.4f}\")\n",
    "\n",
    "# Precision\n",
    "precision_cat = precision_score(y_test, y_pred_studytime_categorical, pos_label=True)\n",
    "print(f\"Precision: {precision_cat:.4f}\")\n",
    "\n",
    "# Recall (for the 'failed' class, which is True or 1) - Metric selected in 2.1\n",
    "recall_cat = recall_score(y_test, y_pred_studytime_categorical, pos_label=True)\n",
    "print(f\"Recall (Selected Metric): {recall_cat:.4f}\")\n",
    "\n",
    "# F1-score\n",
    "f1_cat = f1_score(y_test, y_pred_studytime_categorical, pos_label=True)\n",
    "print(f\"F1-score: {f1_cat:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation of Confusion Matrix:\")\n",
    "print(f\"  True Negatives (Correctly predicted as passing): {cm_cat[0, 0]}\")\n",
    "print(f\"  False Positives (Predicted as failing, but actually passed): {cm_cat[0, 1]}\")\n",
    "print(f\"  False Negatives (Predicted as passing, but actually failed): {cm_cat[1, 0]}\")\n",
    "print(f\"  True Positives (Correctly predicted as failing): {cm_cat[1, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62CqymMsMPkz"
   },
   "source": [
    "##### **2.6.** What is the difference between the two models you built in questions 2.4 and 2.5? Plot the predictions of each model over the range of `studytime` values in the test set. What are the advantages and disadvantages of each approach? **(2.00)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "2jiWG69wyURp",
    "outputId": "f6ad38d8-7006-4ec6-c1d5-0beb5198428b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Preparar os dados para previsão (Range 1 a 4) ---\n",
    "study_vals = [1, 2, 3, 4]\n",
    "\n",
    "# Preparar input para o Modelo Numérico (espera 1 coluna numérica)\n",
    "X_pred_num = pd.DataFrame({'studytime': study_vals})\n",
    "\n",
    "# Preparar input para o Modelo Categórico (espera variáveis dummy)\n",
    "# Assumindo que usou drop_first=True, o nível 1 é a base (tudo 0)\n",
    "# As colunas devem corresponder exatamente às do treino (ex: studytime_2, etc.)\n",
    "X_pred_cat = pd.DataFrame({\n",
    "    'studytime_2': [0, 1, 0, 0],\n",
    "    'studytime_3': [0, 0, 1, 0],\n",
    "    'studytime_4': [0, 0, 0, 1]\n",
    "})\n",
    "\n",
    "# --- 2. Obter as Probabilidades de 'Failed' (classe 1) ---\n",
    "probs_num = model_num.predict_proba(X_pred_num)[:, 1]\n",
    "probs_cat = model_cat.predict_proba(X_pred_cat)[:, 1]\n",
    "\n",
    "# --- 3. Plotar o Gráfico Comparativo ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Linha do Modelo Numérico (tendência suave)\n",
    "plt.plot(study_vals, probs_num, marker='o', linestyle='-', color='blue', label='Modelo Numérico (2.4)', linewidth=2)\n",
    "\n",
    "# Linha do Modelo Categórico (pode ser \"quebrada\")\n",
    "plt.plot(study_vals, probs_cat, marker='s', linestyle='--', color='red', label='Modelo Categórico (2.5)', linewidth=2)\n",
    "\n",
    "plt.title('Model Comparison: Probability of Failure vs. Study Time')\n",
    "plt.xlabel('Studytime (1 to 4)')\n",
    "plt.ylabel('Probability of \"Failed\" (G3 < 10)')\n",
    "plt.xticks(study_vals, ['1 (<2h)', '2 (2-5h)', '3 (5-10h)', '4 (>10h)'])\n",
    "plt.ylim(0, 1) # Probabilidade varia entre 0 e 1\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeAqo-dnMPkz"
   },
   "source": [
    "Modelo 2.4: studytime Numérico (Contínuo)\n",
    "Tratamento: Assume uma relação linear e contínua entre o tempo de estudo e a probabilidade de falha.\n",
    "\n",
    "Performance: Apresentou um Recall extremamente baixo (0.2105) e 15 Falsos Negativos (FN). Isso significa que o modelo falhou drasticamente em identificar a maioria dos alunos que realmente chumbaram, considerando o impacto contínuo de studytime como irrelevante e resultando numa curva de previsão muito plana.\n",
    "\n",
    "Vantagem/Desvantagem: É mais simples, mas a suposição de linearidade revelou-se inadequada para este dataset, comprometendo a capacidade de identificar alunos em risco.\n",
    "\n",
    "Modelo 2.5: studytime Categórico (One-Hot Encoding)\n",
    "Tratamento: Atribui um efeito distinto e não linear a cada categoria de tempo de estudo (1, 2, 3, 4).\n",
    "\n",
    "Performance: Demonstrou um Recall drasticamente superior (0.9474) e apenas 1 Falso Negativo (FN), identificando corretamente quase todos os alunos que falharam. No entanto, a Precision é mais baixa (0.3529), resultando em mais Falsos Positivos (FP).\n",
    "\n",
    "Vantagem/Desvantagem: É mais flexível e capaz de capturar a verdadeira relação discreta entre os grupos de estudo e a falha, mas é mais complexo e gera mais Falsos Positivos.\n",
    "\n",
    "Podemos concluir, que para o objetivo de identificar proactivamente alunos que irão chumbar (priorizando o Recall), o Modelo Categórico é claramente superior. A relação entre studytime e a probabilidade de falha não é linear, e o modelo categórico consegue capturar as diferenças de risco inerentes a cada nível de tempo de estudo. Embora gere mais Falsos Positivos (prever falha para quem passa), minimizar os Falsos Negativos (alunos em risco perdidos) é o fator crucial neste contexto de intervenção."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KFCnGgHMPkz"
   },
   "source": [
    "# Part 3 - Regression (3.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doVtCY_tMPkz"
   },
   "source": [
    "##### In this part of the assignment, you'll approach the problem as a regression task, predicting the actual final grade `G3`. Use the same training and test sets you created in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzmxchJKMPk0"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVzrmWWUMPk0"
   },
   "source": [
    "##### **3.1.** Build a linear regression model to predict the final grade `G3`, using as predictor the feature `G2`. Evaluate the performance of the model on the test set using Mean Squared Error (MSE) and R-squared ($R^2$). **(0.50)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLGcIYoXMPk0",
    "outputId": "652077d1-39f5-4409-c6f2-2eedcebef2b6"
   },
   "outputs": [],
   "source": [
    "# Define features (X) and target (y) for this specific task\n",
    "X_train_G2 = X_train[['G2']]\n",
    "X_test_G2 = X_test[['G2']]\n",
    "y_train_G3 = X_train['G3']\n",
    "y_test_G3 = X_test['G3']\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "linear_model_G2 = LinearRegression()\n",
    "linear_model_G2.fit(X_train_G2, y_train_G3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_G3_G2 = linear_model_G2.predict(X_test_G2)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse_g2 = mean_squared_error(y_test_G3, y_pred_G3_G2)\n",
    "r2_g2 = r2_score(y_test_G3, y_pred_G3_G2)\n",
    "\n",
    "print(\"--- Model Performance Evaluation (G3 predicted by G2) ---\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_g2:.4f}\")\n",
    "print(f\"R-squared (R^2): {r2_g2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLn3TaWsMPk0"
   },
   "source": [
    "##### **3.2.** Print the equation of the model you built in question 3.1. Plot the predictions of the model as a function of the inputs, along with the actual data points from the test set. Interpret the coefficients of the model. **(1.00)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "Np1qGqumMPk1",
    "outputId": "1980f47e-9113-4227-f199-57ddc7c42298"
   },
   "outputs": [],
   "source": [
    "# Print the equation of the model\n",
    "intercept = linear_model_G2.intercept_\n",
    "coefficient = linear_model_G2.coef_[0]\n",
    "print(f\"--- Model Equation (G3 predicted by G2) ---\")\n",
    "print(f\"G3 = {coefficient:.4f} * G2 + {intercept:.4f}\")\n",
    "\n",
    "# Plot the predictions and actual data points\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_test_G2['G2'], y=y_test_G3, label='Actual G3', color='blue', alpha=0.6)\n",
    "sns.lineplot(x=X_test_G2['G2'], y=y_pred_G3_G2, label='Predicted G3', color='red', linestyle='--')\n",
    "plt.title('Actual vs. Predicted G3 based on G2')\n",
    "plt.xlabel('G2 (Second Period Grade)')\n",
    "plt.ylabel('G3 (Final Grade)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PddxzjAMPlM"
   },
   "source": [
    "O interceto (0.2559) representa o valor previsto para G3 quando G2 é igual a 0. Contudo, no contexto das notas, uma nota G2 de 0 pode não ter um significado prático ou pode não ser um cenário real para os dados observados, tornando a interpretação literal do interceto, neste caso, menos relevante. O coeficiente de G2 (0.9929) é o mais importante neste modelo, indicando que para cada aumento de uma unidade na nota G2, a nota final predita G3 aumenta em aproximadamente 0.9929 unidades. Isso sugere uma relação quase linear e fortemente positiva entre a nota do segundo período (G2) e a nota final (G3), onde notas G2 mais altas estão associadas a notas G3 mais altas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Z2ZNYMyMPlN"
   },
   "source": [
    "##### **3.3.** Build a linear regression model to predict the final grade `G3`, using as predictors all features except the variables related to the grades (`G1`, `G2`, `G3`, `failed`). Explain the choices you made. Evaluate the performance of the model on the test set using Mean Squared Error (MSE) and R-squared ($R^2$). Plot the predicted vs actual values of `G3` for the test set. **(1.00)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "7KCL2X00MPlN",
    "outputId": "7b712d2c-f89e-4895-db61-13e49688a67e"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define the target variable for regression (G3) from the original splits\n",
    "y_train_G3 = X_train['G3']\n",
    "y_test_G3 = X_test['G3']\n",
    "\n",
    "# Define features to exclude from the predictors (grades and the 'failed' target)\n",
    "features_to_exclude = ['G1', 'G2', 'G3'] # 'failed' is already dropped when X was created\n",
    "\n",
    "# Create feature sets without the excluded grades\n",
    "X_train_regression = X_train.drop(columns=features_to_exclude)\n",
    "X_test_regression = X_test.drop(columns=features_to_exclude)\n",
    "\n",
    "# Identify categorical and numerical columns for preprocessing\n",
    "categorical_features = X_train_regression.select_dtypes(include='object').columns\n",
    "numerical_features = X_train_regression.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Create a ColumnTransformer to apply OneHotEncoder to categorical features\n",
    "# and leave numerical features untouched\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' # In case there are any other columns not specified\n",
    ")\n",
    "\n",
    "# Apply the preprocessing to the training and test sets\n",
    "X_train_processed = preprocessor.fit_transform(X_train_regression)\n",
    "X_test_processed = preprocessor.transform(X_test_regression)\n",
    "\n",
    "# If the output is a sparse matrix, convert to dense for LinearRegression (if needed)\n",
    "if hasattr(X_train_processed, 'toarray'):\n",
    "    X_train_processed = X_train_processed.toarray()\n",
    "if hasattr(X_test_processed, 'toarray'):\n",
    "    X_test_processed = X_test_processed.toarray()\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "linear_model_all_features = LinearRegression()\n",
    "linear_model_all_features.fit(X_train_processed, y_train_G3)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_G3_all_features = linear_model_all_features.predict(X_test_processed)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse_all = mean_squared_error(y_test_G3, y_pred_G3_all_features)\n",
    "r2_all = r2_score(y_test_G3, y_pred_G3_all_features)\n",
    "\n",
    "print(\"--- Model Performance Evaluation (G3 predicted by all features except grades) ---\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_all:.4f}\")\n",
    "print(f\"R-squared (R^2): {r2_all:.4f}\")\n",
    "\n",
    "# Plot predicted vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test_G3, y=y_pred_G3_all_features)\n",
    "plt.plot([y_test_G3.min(), y_test_G3.max()], [y_test_G3.min(), y_test_G3.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "plt.title('Predicted vs. Actual G3 (All features except grades)')\n",
    "plt.xlabel('Actual G3')\n",
    "plt.ylabel('Predicted G3')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5jIs7-5MPlN"
   },
   "source": [
    "##### **3.4.** In terms of application, what is the difference between building a model to predict student performance using the grades of the previous periods (as in question 3.1) versus using other features (as in question 3.3)? When would each approach be more appropriate? **(1.00)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v853DktMPlO"
   },
   "source": [
    "Usar G2 para prever G3 aproveita a forte correlação entre a nota do segundo período e a nota final, resultando em alta precisão preditiva. Essa abordagem é adequada para previsão de curto prazo ou sistemas de alerta precoce, quando as notas anteriores estão disponíveis. Em contraste, usar todas as outras variáveis não relacionadas a notas, como demográficas, comportamentais, informações escolares, evita o data leakage e permite identificar fatores que influenciam o desempenho além das notas anteriores, embora geralmente tenha menor precisão preditiva. Essa abordagem é mais apropriada para entender os fatores que afetam o desempenho estudantil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwAUecDmMPlO"
   },
   "source": [
    "# Part 4 - Cross-validation (4.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIY5HYOMMPlO"
   },
   "source": [
    "##### **4.1.** Explain the concept of cross-validation and its importance in evaluating machine learning models. **(1.00)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl5FvjZmMPlO"
   },
   "source": [
    "A cross-validation é uma metodologia desenvolvida para avaliar como um modelo de machine learning deverá comportar-se quando aplicado a dados novos e nunca antes observados. Em vez de depender de uma única divisão do conjunto de dados em treino e teste, o método divide repetidamente os dados em vários subconjuntos, permitindo que o modelo seja treinado e avaliado múltiplas vezes. Esta avaliação repetida produz uma estimativa mais estável e fiável da capacidade preditiva do modelo. O seu valor reside no facto de reduzir o risco de conclusões enviesadas causadas por uma divisão de treino/teste excepcionalmente favorável ou desfavorável. Assim, a validação cruzada aumenta a confiança na comparação entre modelos, apoia decisões de seleção e afinação de modelos e oferece uma visão mais clara da capacidade de generalização para além da amostra usada originalmente no treino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_c5VngMMPlO"
   },
   "source": [
    "##### **4.2.** Describe how K-fold cross-validation works. What values of K should be used? Justify. **(1.00)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ik7GrdTYMPlO"
   },
   "source": [
    "A cross-validation K-fold funciona dividindo o conjunto de dados em K blocos de tamanho aproximadamente igual. O modelo é então treinado K vezes de forma independente. Em cada iteração, um dos blocos é deixado de fora para servir como conjunto de validação, enquanto os restantes K−1 blocos constituem o conjunto de treino. Depois de todos os blocos terem desempenhado o papel de validação uma vez, os K resultados obtidos são promediados, fornecendo uma estimativa global do desempenho do modelo. Este método garante que todas as observações são utilizadas tanto para treino como para validação, mas nunca simultaneamente. Quanto à escolha do valor de K, os mais utilizados na prática são 5 e 10. Estes valores representam um equilíbrio adequado entre ter subconjuntos suficientemente representativos e manter o custo computacional em níveis razoáveis. Valores demasiado baixos tendem a produzir estimativas com maior variância, enquanto valores muito elevados, como o leave-one-out cross-validation, podem ser computacionalmente dispendiosos e gerar estimativas instáveis, uma vez que os conjuntos de treino diferem muito pouco entre iterações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Efl4x-_MPlO"
   },
   "source": [
    "##### **4.3.** Explain how K-Fold cross-validation should be done to choose the classification threshold for the logistic regression model of Part 2. **(2.00)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4UFvaI0MPlO"
   },
   "source": [
    "No modelo de regressão logística, a classificação é feita usando o limiar padrão de 0.5, que converte probabilidades previstas em classes. Para definir um limiar mais adequado ao problema e ao conjunto de dados, este valor não deve ser escolhido após treinar o modelo, mas sim determinado dentro de um processo de K-Fold cross-validation aplicado exclusivamente aos dados de treino. A ideia central é avaliar diferentes limiares possíveis enquanto o modelo é validado repetidamente em subconjuntos distintos do próprio treino. Em cada uma das K divisões, o conjunto de treino é dividido em partições com dados internos de treino e de validação. Em seguida, treina-se um modelo de regressão logística com as mesmas configurações. Depois do treino, calculam-se as probabilidades previstas para o subconjunto de validação utilizando model. Estas probabilidades são comparadas com um conjunto de limiares candidatos (por exemplo, de 0.05 até 0.95 com incrementos de 0.05). Para cada limiar, a probabilidade é convertida em classe: prevê-se “fail” (1) quando a probabilidade ultrapassa o limiar e “pass” (0) caso contrário. Para cada limiar são calculadas as métricas de desempenho na validação. A métrica de referência é a sensibilidade, portanto esse será o valor usado para comparar limiares. A sensibilidade obtido em cada fold é registado para cada limiar, e o processo repete-se até completar todos os K folds. No final, para cada limiar obtém-se o valor médio de sensibilidade ao longo das K execuções. O limiar ideal será aquele que apresentar maior sensibilidade. Depois de escolhido esse limiar ótimo, treina-se novamente o modelo final usando todo o conjunto de treino com as mesmas características. Finalmente, as previsões futuras deixam de utilizar o limiar padrão de 0.5 e passam a utilizar o limiar determinado através do processo de K-Fold. Desta forma, a escolha do limiar não utiliza o conjunto de teste, evita data leakage e assegura que o limiar selecionado realmente reflete a capacidade de generalização do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STfDvxmfMPlO"
   },
   "source": [
    "# Part 5 - GitHub (1.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmHU23m1MPlO"
   },
   "source": [
    "##### **5.1.** Describe how your group used GitHub to track changes throughout the project and explain why maintaining a clear change history is important in data analysis and machine learning work. **(1.00)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww0JC4NcMPlP"
   },
   "source": [
    "Ao longo do projeto, o grupo utilizou o GitHub como plataforma central para gerir versões e acompanhar a evolução do trabalho. As contribuições individuais foram feitas através de commits regulares, sempre acompanhados de mensagens descritivas que identificavam claramente as alterações realizadas, como a introdução de novos métodos, correções de erros, melhoria de visualizações ou reorganização da estrutura do projeto. Sempre que necessário, foram criados branches separados para desenvolver funcionalidades ou análises específicas sem interferir com a versão principal, assegurando que o processo de integração era organizado e controlado. O uso de pull requests permitiu rever e discutir alterações antes de as incorporar na versão final, garantindo consistência e qualidade no código. Manter um histórico de alterações claro é fundamental em projetos de análise de dados e machine learning porque estes exigem rastreabilidade, reprodutibilidade e controlo rigoroso das diferentes versões dos modelos e dos dados. Um histórico bem estruturado permite compreender a origem de resultados, identificar rapidamente quando e onde surgiram falhas ou divergências e justificar decisões metodológicas tomadas ao longo do desenvolvimento. Além disso, facilita o trabalho colaborativo, evitando conflitos, perdas de código e duplicação de esforços."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
